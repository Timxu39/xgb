# 导入库
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
from xgboost import plot_importance
 
### 载入数据
digits = datasets.load_digits()
 
### data analysis
print(digits.data.shape)
print(digits.target.shape)
 
### 划分训练集测试集
x_train,x_test,y_train,y_test = train_test_split(digits.data, digits.target, test_size = 0.3,random_state = 33)
### 训练模型
model = XGBClassifier(learning_rate=0.1,
                      n_estimators=100,         # 树的个数--100棵树建立xgboost
                      max_depth=6,               # 树的深度
                      min_child_weight = 1,      # 叶子节点最小权重
                      gamma=0.,                  # 惩罚项中叶子结点个数前的参数
                      subsample=0.8,             # 随机选择80%样本建立决策树
                      colsample_btree=0.8,       # 随机选择80%特征建立决策树
                      objective='multi:softmax', # 指定损失函数
                      scale_pos_weight=1,        # 解决样本个数不平衡的问题
                      random_state=27            # 随机数
                      )
# 拟合
model.fit(x_train, y_train, eval_set = [(x_test,y_test)], eval_metric = "mlogloss", early_stopping_rounds = 10,
          verbose = True)
 
### 特征重要性
fig,ax = plt.subplots(figsize=(15,15))
plot_importance(model, height=0.5, ax=ax, max_num_features=64)
plt.show()
 
### 预测
y_pred = model.predict(x_test)
 
### 模型正确率
accuracy = accuracy_score(y_test,y_pred)
print("准确率: %.2f%%" % (accuracy*100.0))



交叉验证
xgb.cv(params = list(), data, nrounds, nfold, label = NULL, missing = NA, prediction = FALSE, showsd = TRUE,
	metrics = list(), obj = NULL, feval = NULL, stratified = TRUE, folds = NULL, verbose = TRUE
	print_every_n = 1L, early_stopping_rounds = NULL, maximize = NULL, callbacks = list(), ...
